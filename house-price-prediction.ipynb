{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üè° House Price Prediction ‚Äì Ames, Iowa\n### üìò Kaggle: Advanced Regression Techniques\n\n## 1Ô∏è‚É£ Business Problem\nA homebuyer wants to estimate the price of their dream house but does not know how each feature influences its value.  \nIn this project, we aim to understand the factors affecting house prices and build a predictive model capable of estimating a home's sale price accurately.\n\n---\n\n## 2Ô∏è‚É£ Dataset Story\nThis project uses the Kaggle competition dataset **\"House Prices: Advanced Regression Techniques.\"**\n\n- üìä Train Set: 1,460 houses  \n- üìä Test Set: 1,459 houses  \n- üß© Features: 79 variables describing structural, locational, and quality attributes  \n- üéØ Target: `SalePrice`\n\nThe dataset includes:  \n- Lot size and shape  \n- House quality and overall condition  \n- Living areas (basement, first floor, second floor)  \n- Garage features  \n- Year built and remodeled  \n- Neighborhood characteristics  \n- External materials and more\n\n---\n\n","metadata":{}},{"cell_type":"markdown","source":"\n# üîß 1. Import Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport time\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n\n\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üì• 2. Loading the Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\ndf = pd.concat([train, test], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.shape ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç 3. Exploratory Data Analysis (EDA)\n","metadata":{}},{"cell_type":"code","source":"def check_df(dataframe, head=5):\n    print('##################### Shape #####################')\n    print(dataframe.shape)\n    print('##################### Types #####################')\n    print(dataframe.dtypes)\n    print('##################### Head #####################')\n    print(dataframe.head(head))\n    print('##################### Tail #####################')\n    print(dataframe.tail(head))\n    print('##################### NA #####################')\n    print(dataframe.isnull().sum())\n    print('##################### Quantiles #####################')\n    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî¢ 4. Classifying Variables: Numerical vs Categorical","metadata":{}},{"cell_type":"code","source":"def grab_col_names(dataframe , cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"] \n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car] \n\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"] \n\n    num_cols = [col for col in num_cols if col not in num_but_cat] \n    \n    print(f\"Observations: {dataframe.shape[0]}\") \n    print(f\"Variables: {dataframe.shape[1]}\") \n    print(f'cat_cols: {len(cat_cols)}') \n    print(f'num_cols: {len(num_cols)}') \n    print(f'cat_but_car: {len(cat_but_car)}') \n    print(f'num_but_cat: {len(num_but_cat)}') \n\n\n    return cat_cols, num_cols, cat_but_car, num_but_cat\n\ncat_cols, num_cols, cat_but_car,  num_but_cat = grab_col_names(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_but_car","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_but_cat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî† 5. Analysis of Categorical Variables","metadata":{}},{"cell_type":"code","source":"def cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        'Ratio': 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n    print('##########################################')\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show(block=True)\n\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üßÆ 6. Understanding Numerical Features","metadata":{}},{"cell_type":"code","source":"def num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show(block=True)\n\nfor col in num_cols:\n    num_summary(df, col, plot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 7. Analysis of Categorical Variables by Target","metadata":{}},{"cell_type":"code","source":"def target_summary_with_cat(dataframe, target, categorical_col, plot=False):\n    print(pd.DataFrame({'TARGET_MEAN': dataframe.groupby(categorical_col)[target].mean()}), end='\\n\\n\\n')\n    if plot:\n        sns.barplot(x=categorical_col, y=target, data=dataframe)\n        plt.show(block=True)\n\nfor col in cat_cols:\n    target_summary_with_cat(df, 'SalePrice', col, plot=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 8. Analysis of Numerical Variables by Target","metadata":{}},{"cell_type":"code","source":"def target_summary_with_num(dataframe, target, numerical_col, plot=False):\n    print(pd.DataFrame({numerical_col+'_mean': dataframe.groupby(target)[numerical_col].mean()}), end='\\n\\n\\n')\n    if plot:\n        sns.barplot(x=target, y=numerical_col, data=dataframe)\n        plt.show(block=True)\n\nfor col in num_cols:\n    target_summary_with_cat(df, 'SalePrice', col, plot=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìà 9. Correlation Heatmap & Analysis","metadata":{}},{"cell_type":"code","source":"def high_correlated_cols(dataframe, plot=False, corr_th=0.70):\n    # Sadece numerik kolonlarƒ± al\n    df_numeric = dataframe.select_dtypes(include=[np.number])\n    \n    # Korelasyon matrisi\n    corr = df_numeric.corr()\n    cor_matrix = corr.abs()\n\n    # √úst √º√ßgen\n    upper_triangle_matrix = cor_matrix.where(\n        np.triu(np.ones(cor_matrix.shape), k=1).astype(bool)\n    )\n\n    # E≈üik √ºzerindeki korelasyonlar\n    drop_list = [col for col in upper_triangle_matrix.columns \n                 if any(upper_triangle_matrix[col] > corr_th)]\n\n    # Plot opsiyonu\n    if plot:\n        plt.figure(figsize=(12, 12))\n        sns.heatmap(corr, cmap=\"RdBu\", annot=False)\n        plt.show()\n\n    return drop_list\n\n\nhigh_correlated_cols(df, plot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_num = df[num_cols]\n\ncorr_features = pd.DataFrame(\n    df_num.corr()['SalePrice']\n    .drop('SalePrice', axis=0)\n    .sort_values(ascending=False)\n)\n\ncorr_features.apply(lambda x: round(x, 3) * 100).head(50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def high_correlated_cols(dataframe, head=10):\n    # sadece numerik kolonlarƒ± al\n    df_num = dataframe.select_dtypes(include=[np.number])\n    \n    # korelasyon matrisi\n    corr_matrix = df_num.corr().abs()\n    \n    # √ºst √º√ßgen ile y√ºksek korelasyon √ßiftleri\n    corr_pairs = (\n        corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        .stack()\n        .sort_values(ascending=False)\n        .head(head)\n    )\n    \n    return corr_pairs\n\nhigh_correlated_cols(df, 20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"SalePrice\"].hist(bins=100)\nplt.show(block=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.log1p(df['SalePrice']).hist(bins=50)\nplt.show(block=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quantile_summary(df):\n    num_df = df.select_dtypes(include=[\"int64\", \"float64\"])\n    q_df = num_df.quantile([0.01, 0.05, 0.50, 0.95, 0.99]).T\n    q_df.columns = [\"Q01\", \"Q05\", \"Median\", \"Q95\", \"Q99\"]\n    q_df[\"IQR\"] = q_df[\"Q95\"] - q_df[\"Q05\"]\n    q_df[\"Range99\"] = q_df[\"Q99\"] - q_df[\"Q01\"]\n    return q_df.sort_values(\"Range99\", ascending=False)\nquantile_summary(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üõ†Ô∏è 10. Feature Engineering\n\nIn this section, we will apply several feature engineering steps to enhance the quality and predictive power of the dataset.  \nThese steps help the model better understand patterns and relationships within the data.\n\n---\n\n## üß™ 10.1 Feature Extraction  \nNew features will be created from existing variables to strengthen the model‚Äôs learning capability.  \nThis may include transformations, ratios, categorization, or domain-driven feature creation.\n\n\n\n---\n\n## üîç 10.2 Missing Values Detection  \nWe identify and handle missing values to prevent biases and errors during model training.\n\n---\n\n## üö® 10.3 Outlier Detection  \nOutliers can negatively affect model performance.  \nWe will detect and treat outliers in numerical variables using appropriate statistical methods.","metadata":{}},{"cell_type":"markdown","source":"# üß™ 10.1 Feature Extraction ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ===============  FLOOR FEATURES (Kat Alanlarƒ±) ===============\n# ============================================================\ndf[\"TotalFlrSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\ndf[\"HasSecondFloor\"] = (df[\"2ndFlrSF\"] > 0).astype(int)\n# ============================================================\n# ================= BASEMENT FEATURES (BODRUM) ===============\n# ============================================================\ndf[\"BsmtFinishedRatio\"] = (df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"]) / (df[\"TotalBsmtSF\"] + 1)\ndf[\"BasementFinished\"] = ((df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"]) > 0).astype(int)\ndf[\"HasBasement\"] = (df[\"TotalBsmtSF\"] > 0).astype(int)\n# ============================================================\n# =================== GARAGE FEATURES (GARAJ) =================\n# ============================================================\ndf[\"HasGarage\"] = (df[\"GarageArea\"] > 0).astype(int)\ndf[\"GarageCapacityQuality\"] = df[\"GarageCars\"] * df[\"GarageArea\"]\nfinish_map = {\"Fin\":3, \"RFn\":2, \"Unf\":1, np.nan:0}\ndf[\"GarageFinishScore\"] = df[\"GarageFinish\"].map(finish_map)\ndf[\"GarageFinishArea\"] = df[\"GarageArea\"] * df[\"GarageFinishScore\"]\n# ============================================================\n# =================== PORCH FEATURES (VERANDA) ===============\n# ============================================================\ndf[\"TotalPorchSF\"] = (\n    df[\"OpenPorchSF\"] +\n    df[\"EnclosedPorch\"] +\n    df[\"3SsnPorch\"] +\n    df[\"ScreenPorch\"]\n)\ndf[\"HasPorch\"] = (df[\"TotalPorchSF\"] > 0).astype(int)\ndf[\"PorchCount\"] = (\n    (df[\"OpenPorchSF\"] > 0).astype(int) +\n    (df[\"EnclosedPorch\"] > 0).astype(int) +\n    (df[\"3SsnPorch\"] > 0).astype(int) +\n    (df[\"ScreenPorch\"] > 0).astype(int)\n)\n# ============================================================\n# ===================== MISC FEATURES =========================\n# ============================================================\ndf[\"HasMiscVal\"] = (df[\"MiscVal\"] > 0).astype(int)\ndf[\"HasPool\"] = (df[\"PoolArea\"] > 0).astype(int)\n# ============================================================\n# =================== AGE & REMODEL FEATURES ==================\n# ============================================================\ndf[\"RemodelAge\"] = df[\"YearRemodAdd\"] - df[\"YearBuilt\"]\ndf[\"AgeSinceRemodel\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\ndf[\"AgeSinceBuilt\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n# ============================================================\n# ================= QUALITY-BASED FEATURES ====================\n# ============================================================\ndf[\"QualTotalSF\"] = df[\"OverallQual\"] * df[\"TotRmsAbvGrd\"]\ndf[\"QualLivingRatio\"] = df[\"OverallQual\"] / (df[\"GrLivArea\"] + 1)\ndf[\"QualMinusCond\"] = df[\"OverallQual\"] - df[\"OverallCond\"]\n\n\ndrop_list = [\"1stFlrSF\" , \"2ndFlrSF\" , \"BsmtFinSF1\" , \"BsmtFinSF2\" , \"GarageArea\" ,\"GarageYrBlt\" ,   \"GarageCars\", \"MiscVal\" , \"MasVnrArea\" , \"WoodDeckSF\" , \"OpenPorchSF\" , \"EnclosedPorch\" , \"3SsnPorch\" ]\ndf.drop(drop_list, axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"MSZoning\"].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç 10.2 Missing Values Detection  ","metadata":{}},{"cell_type":"code","source":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_cols = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",\n           \"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PoolQC\",\"Fence\",\"MiscFeature\" , \"MasVnrType\"]\nfor col in no_cols:\n    df[col].fillna(\"No\",inplace=True)\n\nmissing_values_table(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quick_missing_imp(data, num_method=\"median\", cat_length=20, target=\"SalePrice\"):\n    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  \n\n    temp_target = data[target]\n\n    print(\"# BEFORE\")\n    print(data[variables_with_na].isnull().sum(), \"\\n\\n\")  \n\n    \n    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n\n    \n    if num_method == \"mean\":\n        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n    \n    elif num_method == \"median\":\n        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n\n    data[target] = temp_target\n\n    print(\"# AFTER \\n Imputation method is 'MODE' for categorical variables!\")\n    print(\" Imputation method is '\" + num_method.upper() + \"' for numeric variables! \\n\")\n    print(data[variables_with_na].isnull().sum(), \"\\n\\n\")\n\n    return data\n\ndf = quick_missing_imp(df, num_method=\"median\", cat_length=17)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üö® 10.3 Outlier Detection  ","metadata":{}},{"cell_type":"code","source":"cat_cols, num_cols, cat_but_car,  num_but_cat = grab_col_names(df)\ntrue_outliers = []\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        true_outliers.append(col)\n        return True\n    else:\n        return False\n\nfor col in num_cols:\n    if col != \"SalePrice\":\n        print(col, ':', check_outlier(df, col))\n\ndef plot_outlier_columns(df, true_cols):\n    \"\"\"\n    true_cols i√ßindeki kolonlarƒ± otomatik olarak\n    boxplot + histogram + KDE ile √ßizer.\n    \"\"\"\n    for col in true_cols:\n        plt.figure(figsize=(16,5))\n\n        # ---------------------------\n        # BOX PLOT\n        # ---------------------------\n        plt.subplot(1, 2, 1)\n        sns.boxplot(x=df[col], color=\"orange\")\n        plt.title(f\"Boxplot ‚Äî {col}\", fontsize=14)\n        plt.xlabel(col)\n\n        # ---------------------------\n        # HISTOGRAM + KDE\n        # ---------------------------\n        plt.subplot(1, 2, 2)\n        sns.histplot(df[col], kde=True, bins=30, color=\"skyblue\")\n        plt.title(f\"Histogram ‚Äî {col}\", fontsize=14)\n        plt.xlabel(col)\n        plt.ylabel(\"Frequency\")\n\n        plt.tight_layout()\n        plt.show()\n\nplot_outlier_columns(df ,true_outliers )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"winsor_cols = [\"LotFrontage\", \"GrLivArea\", \"TotalBsmtSF\", \"TotalFlrSF\"]\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n\ndf[\"LotArea_Log\"] = np.log1p(df[\"LotArea\"])\n\nheavy_skew_cols = [\"ScreenPorch\", \"PoolArea\", \"TotalPorchSF\", \"LowQualFinSF\"]\n\nfor col in heavy_skew_cols:\n    df[col + \"_Binary\"] = (df[col] > 0).astype(int)\n    df[col + \"_Log\"] = np.log1p(df[col])\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in num_cols:\n    if col != \"SalePrice\":\n        print(col, ':', check_outlier(df, col))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî§ 11. Rare Analysis","metadata":{}},{"cell_type":"code","source":"def rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, ':', len(dataframe[col].value_counts()))\n        print(pd.DataFrame({'COUNT': dataframe[col].value_counts(),\n                            'RATIO': dataframe[col].value_counts() / len(dataframe),\n                            'TARGET_MEAN': dataframe.groupby(col)[target].mean()}), end='\\n\\n\\n')\n\nrare_analyser(df, \"SalePrice\", cat_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() / len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n    return temp_df\n\nrare_encoder(df, 0.01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî§ 12. Encoding","metadata":{}},{"cell_type":"code","source":"def binary_cols(dataframe):\n    binary_cols = [col for col in dataframe.columns if dataframe[col].dtype not in ['int64', 'float64'] and dataframe[col].nunique() <= 2]\n    return binary_cols\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nbinary_cols = binary_cols(df)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in binary_cols:\n    df = label_encoder(df, col)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndf = one_hot_encoder(df, cat_cols, drop_first=True)\n\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns = df.columns.str.replace(\" \", \"_\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(\"Neighborhood\", axis=1, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ü§ñ 13. Modeling","metadata":{}},{"cell_type":"code","source":"train_df = df[df['SalePrice'].notnull()].copy()\ntest_df  = df[df['SalePrice'].isnull()].copy()\n\ntrain_df[\"SalePrice_Log\"] = np.log1p(train_df[\"SalePrice\"])\n\n\ny = train_df[\"SalePrice_Log\"]\nX = train_df.drop([\"Id\", \"SalePrice\", \"SalePrice_Log\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          #('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\nrmse_scores = []\nexecution_times = []\n\nfor name, regressor in models:\n    start_time = time.time()\n\n    # Fit the model\n    regressor.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = regressor.predict(X_test)\n\n    # Calculate RMSE\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    rmse_scores.append(rmse)\n\n    # Calculate the execution time of the model\n    execution_time = time.time() - start_time\n    execution_times.append(execution_time)\n\n    print(f\"RMSE: {round(rmse, 4)} ({name})\")\n    print(f\"Execution Time: {round(execution_time, 2)} seconds\\n\")\n\n# Plot RMSE scores\nplt.figure(figsize=(12, 8))\n# Exclude LR from the plot\nfiltered_scores = [score for name, score in zip([name for name, _ in models], rmse_scores) if name != 'LR']\nplt.bar([name for name, _ in models if name != 'LR'], filtered_scores)\nplt.xlabel(\"Model\")\nplt.ylabel(\"RMSE\")\nplt.title(\"Model Performance (RMSE)\")\nplt.show()\n\n# Plot execution times\nplt.figure(figsize=(12, 8))\nplt.bar([name for name, _ in models], execution_times)\nplt.xlabel(\"Execution Time (seconds)\")\nplt.ylabel(\"Model\")\nplt.title(\"Execution Times for Different Models\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_test = regressor.predict(X_test)\n\n# Log‚Äôdan geri d√∂n√º≈ü√ºm\ny_pred_test_original_scale = np.expm1(y_pred_test)\ny_test_original_scale = np.expm1(y_test)\n\n# RMSE hesapla\nrmse_original_scale = np.sqrt(np.mean((y_pred_test_original_scale - y_test_original_scale) ** 2))\n\nprint(f\"RMSE in original scale: {round(rmse_original_scale, 4)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_df = df[df['SalePrice'].notnull()].copy()\ntest_df  = df[df['SalePrice'].isnull()].copy()\n\ny = np.log1p(train_df['SalePrice'])\nX = train_df.drop([\"Id\", \"SalePrice\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=17\n)\n\n\n\nmodels = [\n    (\"GBM\", GradientBoostingRegressor()),\n    (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n    (\"LightGBM\", LGBMRegressor()),\n    (\"CatBoost\", CatBoostRegressor(verbose=False))\n]\n\nparam_grids = {\n    \"GBM\": {\"n_estimators\": [100, 200], \"max_depth\": [5, 7], \"learning_rate\": [0.01, 0.1]},\n    \"XGBoost\": {\"n_estimators\": [100, 200], \"max_depth\": [5, 7], \"learning_rate\": [0.01, 0.1]},\n    \"LightGBM\": {\"n_estimators\": [100, 200], \"max_depth\": [5, 7], \"learning_rate\": [0.01, 0.1]},\n    \"CatBoost\": {\"iterations\": [100, 200], \"depth\": [5, 7], \"learning_rate\": [0.01, 0.1]}\n}\n\nrmse_values = []\nexecution_times = []\nmodel_names = []\n\n\n\nfor name, regressor in models:\n    print(f\"Hyperparameter Tuning for {name}:\")\n\n    start_time = time.time()\n    grid_search = GridSearchCV(\n        regressor, \n        param_grid=param_grids[name], \n        cv=5, \n        n_jobs=-1\n    )\n    grid_search.fit(X_train, y_train)\n    end_time = time.time()\n\n    execution_time = end_time - start_time\n    best_model = grid_search.best_estimator_\n\n    print(f\"Best Parameters: {grid_search.best_params_}\")\n    print(f\"Execution Time: {execution_time:.4f} seconds\")\n\n    # RMSE in original scale\n    y_pred = best_model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\n\n    print(f\"RMSE: {rmse:.4f}\")\n    print(\"---------------\")\n\n    rmse_values.append(rmse)\n    execution_times.append(execution_time)\n    model_names.append(name)\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(model_names, rmse_values, color='steelblue')\nplt.xlabel('Model')\nplt.ylabel('RMSE')\nplt.title('Top 4 Model Performance (RMSE)')\nplt.xticks(rotation=45)\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(model_names, execution_times, color='darkorange')\nplt.xlabel('Model')\nplt.ylabel('Execution Time (seconds)')\nplt.title('Execution Times ‚Äì Top 4 Models')\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final Prediction Model\nfinal_model = best_model\n\n# Make predictions on the test set using the final model\ny_final_pred = final_model.predict(X_test)\nfinal_y_pred = np.expm1(y_final_pred)\nfinal_y_test = np.expm1(y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataFrame with the predicted prices and true prices\nresults = pd.DataFrame({'Predicted Price': final_y_pred, 'True Price': final_y_test})\n\n# Calculate the difference between the true prices and predicted prices and add a new column\nresults['Difference'] = results['True Price'] - results['Predicted Price']\n\n# Display the results\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr2_original = r2_score(final_y_test, final_y_pred)\nprint(\"R2 Score (Original Scale):\", round(r2_original, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataFrame with the predicted prices and true prices\nresults = pd.DataFrame({'Predicted Price': final_y_pred, 'True Price': final_y_test})\n\n# Create a line plot\nsns.lineplot(data=results)\n\n# Label the axes\nplt.xlabel('Sample')\nplt.ylabel('Price')\n\n# Set the title\nplt.title('Predicted Prices vs. True Prices')\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_importance(model, features, num=50, save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show(block=True)\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(final_model, X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_y_pred = np.expm1(y_final_pred)\nfinal_y_test = np.expm1(y_test)\n\n# Kaggle Metric: RMSE(log(pred), log(actual))\nkaggle_rmse = np.sqrt(mean_squared_error(\n    np.log1p(final_y_test), \n    np.log1p(final_y_pred)\n))\n\nprint(\"Kaggle Metric RMSE (log scale):\", round(kaggle_rmse, 5))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}